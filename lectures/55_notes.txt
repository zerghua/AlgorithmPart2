Introduction to Data Compression
----------------------------------------------------------
compression reduces the size of a file:
- to save space when storing it.
- to save time when transmitting it.
- most of files have lots of redundancy.


who needs compression?
- Moore's law: transistors on chip double very 18-24 months
- Parkinson's law: data expends to fill space available.
- text, image, sound, video...


Applications.
Generic file compression
- Files: GZIP, BZIP, 7z
- Archivers: PKZIP
- File system: NTFS, HFS + ZFS

Multimedia.
- Image: GIF, JPEG
- Sound: MP3
- Video: MPEG, DivX, HDTV

Communication.
- ITU-T 4 Group 3 Fax
- V.42bis modern
- skype


Database. Google, Facebook, ...


Lossless compression and expansion
Message. Binary data B we want to compress.
Compress. Generates a "compressed" representation C(B).
Expand. Reconstructs original bitstream B.


Compression ratio. Bits in C(B) / bits in B.

Ex. 50-75% better compression ratio for natural language.


Data representation: genomic code
Genome. String over the alphabet{A, C, T, G}
Goal. Encode and N-character genome: ATAGATC...

Standard ASCII encoding: 8 bits per char
Two-bit encoding: 2 bits per char.
Fixed-length code. K-bit code supports alphabet of size 2^k.


data representation for 12/31/1999
standard ascii, each char is 8 bits, so 10 char = 10*8=80bits
store each number as int, each int is 32 bits, so 3 int=3*24=96bits
since month in range [1,12], and day in range [1,31], so we
only need to store the last few 4 for month(4^2=16), last 5 bits
for day(2^5=32), 12 bits for year(2^12=4096), so 21 bits
with 3 extra bits for byte alignment, so total 24 bits.


US patent 5,533,051 on "Methods for data compression", which is
capable of compression all files.



Run-Length Coding
----------------------------------------------------------
Simple type of redundancy in a bitstream. Long runs of repeated bits.

representation. 4-bit counts to represent alternating runs of
0s and 1s: 15 0s, then 7 1s, then 7s, then 11 1s.
15 7 7 11 -> 16 bits instead of 40


Q. How many bits to store the counts?
A. We'll use 8(but 4 in the example above)

Q. What to do when run length exceeds max count?
A. If longer than 255, intersperse runs of length 0.





Huffman Compression
----------------------------------------------------------
Variable-length codes
Q. How do we avoid ambiguity?
A. Ensure that no codeword is a prefix of another.

Ex.1. Fixed-length code.
Ex.2. Append special stop char to each codeword.
Ex.3. General prefix-free code.


Q. How represent the prefix-free code?
A. A binary trie.
- chars in leaves.
- codeword is path from root to leaf.


Prefix-free codes: compression and expansion.
Compression
- Method 1: start at leaf; follow path up to the root; print
bits in reverse.
- Method 2: create ST of key-value pairs.

Expansion
- start at root.
- Go left if bit is 0; go right if 1
- If leaf node, print char and return to root.



private static class Node implements Comparable<Node>{
    private char ch;
    private int freq;
    private final Node left, right;

    public Node(char ch, int  freq, Node left, Node right){
        this.ch = ch;
        this.freq = freq;
        this.left = left;
        this.right = right;
    }

    public boolean isLeaf(){
        return left == null && right == null;
    }

    public int compareTo(Node that){
        return this.freq - that.freq;
    }
}

public void expand(){
    Node root = readTrie();
    int N = BinaryStdIn.readInt();

    for(int i=0; i<N; i++){
        Node x = root;
        while(!x.isLeaf()){
            if(!BinaryStdIn.readBoolean()) x = x.left;
            else x = x.right;
        }
        BinaryStdOut.write(x.ch, 8);
    }
    BinaryStdOut.close();
}

Runtime: linear to N.


How to transmit?
Q. How to write the trie?
A. Write preorder traversal of trie; mark leaf and internal
nodes with a bit.

private static void writeTrie(Node x){
    if(x.isLeaf()){
        BinaryStdOut.write(true);
        BinaryStdOut.write(x.ch, 8);
        return;
    }
    BinaryStdOut.write(false);
    writeTrie(x.left);
    writeTrie(x.right);
}


Note. If message is long, overhead of transmitting trie is small.


Q. How to read in the trie?
A. Reconstruct from preorder traversal of trie.


private static Node readTrie(){
    if(BinaryStdIn.readBoolean()){
        char c = BinaryStdIn.readChar(8);
        return new Node(c, 0, null, null);
    }
    Node x = readTrie();
    Node y = readTrie();
    return new Node('\0',0,x,y);
}



Q. How to find best prefix-free code?
Shannon-Fano algorithm. Not optimal.



Huffman algorithm
- count frequency for each character in input.
- start with node corresponding to each character with weight
equal to frequency.
- repeat until single trie formed:
    - select two tries with min weight
    - Merge into single trie with cumulative weight.


Applications:
JPeg, PDF, mp3, Divx, Gzip.


Constructing a Huffman encoding trie.
private static Node buildTrie(int[] freq){
    MinPQ<Node> pq = new MinPQ<Node>();
    for(char i=0; i<R; i++){
        // initialize PQ with singleton tries
        if(freq[i] > 0){
            pq.insert(new Node(i, freq[i], null, null));
        }
    }

    while(pq.size() > 1){
        Node x = pq.delMin();
        Node y = pq.delMin();
        Node parent = new Node('\0', x.freq + y.freq, x, y);
        pq.insert(parent);
    }
    return pq.delMin();
}


Proposition. Huffman produces an optimal prefix-free code.

Implementation.
- Pass 1. tabulate char frequencies and build trie.
- Pass 2. encode file by traversing trie or lookup table.

Running time. Using a binary heap -> N + RlogR
N=input size
R=alphabet size

Can we do better?




LZW Compression
----------------------------------------------------------







